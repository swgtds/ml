# from scratch

import numpy as np
from collections import Counter
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Simple Decision Tree (very basic version)
class DecisionTree:
    def __init__(self, depth=1):
        self.depth = depth
        self.feature_index = None
        self.threshold = None
        self.left = None
        self.right = None
        self.label = None

    def fit(self, X, y):
        if self.depth == 0 or len(set(y)) == 1:
            self.label = Counter(y).most_common(1)[0][0]
            return

        n_samples, n_features = X.shape
        best_gain = -1
        for i in range(n_features):
            thresholds = np.unique(X[:, i])
            for t in thresholds:
                left_idx = X[:, i] <= t
                right_idx = X[:, i] > t
                if len(set(y[left_idx])) > 0 and len(set(y[right_idx])) > 0:
                    gain = self.information_gain(y, y[left_idx], y[right_idx])
                    if gain > best_gain:
                        best_gain = gain
                        self.feature_index = i
                        self.threshold = t
                        best_left_idx = left_idx
                        best_right_idx = right_idx

        if best_gain == -1:
            self.label = Counter(y).most_common(1)[0][0]
            return

        self.left = DecisionTree(depth=self.depth - 1)
        self.right = DecisionTree(depth=self.depth - 1)
        self.left.fit(X[best_left_idx], y[best_left_idx])
        self.right.fit(X[best_right_idx], y[best_right_idx])

    def predict(self, x):
        if self.label is not None:
            return self.label
        if x[self.feature_index] <= self.threshold:
            return self.left.predict(x)
        else:
            return self.right.predict(x)

    def information_gain(self, parent, left, right):
        def entropy(y):
            hist = np.bincount(y)
            ps = hist / len(y)
            return -np.sum([p * np.log2(p) for p in ps if p > 0])
        return entropy(parent) - (len(left)/len(parent))*entropy(left) - (len(right)/len(parent))*entropy(right)

# Random Forest
class RandomForest:
    def __init__(self, n_trees=10, max_depth=2, sample_size=0.8):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.sample_size = sample_size
        self.trees = []

    def bootstrap_sample(self, X, y):
        n_samples = int(self.sample_size * X.shape[0])
        indices = np.random.choice(X.shape[0], n_samples, replace=True)
        return X[indices], y[indices]

    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            tree = DecisionTree(depth=self.max_depth)
            X_sample, y_sample = self.bootstrap_sample(X, y)
            tree.fit(X_sample, y_sample)
            self.trees.append(tree)

    def predict(self, X):
        tree_preds = np.array([[tree.predict(x) for x in X] for tree in self.trees])
        return [Counter(col).most_common(1)[0][0] for col in tree_preds.T]

# Load data
data = load_iris()
X = data.data
y = data.target

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train Random Forest
rf = RandomForest(n_trees=10, max_depth=3)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

# Evaluate
print("Accuracy (from scratch):", accuracy_score(y_test, y_pred))

















#using scikit learn

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

# Predict on test data
y_pred = rf_model.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
